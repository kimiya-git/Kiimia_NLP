Wikipedia Text Classification: Geographic vs Non-Geographic

ğŸ“Œ Project Overview
This project classifies English Wikipedia texts into two categories:

Geographic (e.g., articles about countries, cities, rivers)

Non-Geographic (e.g., articles about people, events, concepts)

The pipeline combines NLP techniques and machine learning models to automate this classification.

ğŸ§  Problem Statement
Given a Wikipedia article (English text), the system predicts its category:

Class 1: Geographic

Class 2: Non-Geographic

Key challenges include feature extraction, handling noisy text, and model interpretability.

ğŸ› ï¸ Technologies Used
Python 3.10+

NLTK â€“ Tokenization, stopwords, stemming/lemmatization

scikit-learn â€“ Naive Bayes, Logistic Regression, TF-IDF

Wikipedia API â€“ Data collection

WordNet/Snowball â€“ Text normalization

ğŸ” Pipeline Overview
1. Data Collection
Fetch articles using the MediaWiki API.

Manual labeling for training data.

2. Text Preprocessing
Tokenization, stopword removal , lemmatization.

3. Feature Extraction
TF-IDF representations.

4. Model Training

Logistic Regression

5. Evaluation
Metrics: Accuracy, Precision, Recall, F1-score.

Output: confusion_matrix.png for visualization.

6. Deployment
GitHub-hosted code with modular scripts (see Project Structure).

Klimia_NLP/  
â”œâ”€â”€ data/                    # Raw and processed datasets  
â”œâ”€â”€ configs/  
â”‚   â””â”€â”€ preprocessing_config.yaml  # Configuration for text cleaning  
â”œâ”€â”€ src/  
â”‚   â”œâ”€â”€ data.py             # Data loading/utils  
â”‚   â”œâ”€â”€ preprocessing.py    # Text normalization  
â”‚   â”œâ”€â”€ feature_extraction.py  # BoW/TF-IDF implementation  
â”‚   â”œâ”€â”€ model.py            # ML models (Naive Bayes)  
â”‚   â”œâ”€â”€ prediction_pipeline.py  # End-to-end inference  
â”‚   â””â”€â”€ utils.py            # Helper functions  
â”œâ”€â”€ model.pkl               # Serialized trained model  
â”œâ”€â”€ main.py                 # Entry point for training/evaluation  
â””â”€â”€ requirements.txt        # Dependencies (pip install -r requirements.txt)

Task 2: Extended Assignment
Klimia_NLP/assignment2/     # Additional experiments or secondary tasks  

ğŸš€ Getting Started
1. Install dependencies:
pip install -r requirements.txt  

2. Run the pipeline:

python main.py  # preprocess the input files

python model.py   # Trains model and saves outputs 

python prediction_pipeline.py   # given a sample text if classifies
