
# Wikipedia Text Classification: Geographic vs Non-Geographic

## ğŸ“Œ Project Overview

This project addresses the task of classifying English Wikipedia texts into two categories:
- **Geographic**
- **Non-Geographic**

The classification is based on natural language processing (NLP) techniques and machine learning models. The goal is to build a pipeline that can automatically determine whether a given Wikipedia article is related to geographic content.

## ğŸ§  Problem Statement

Given a text input (in English), the system must assign it to one of two classes:
- `Geographic`
- `Non-Geographic`

The classification is based on features extracted from the text, which may include annotated keywords, linguistic patterns, and statistical representations.

## ğŸ› ï¸ Technologies Used

- **Python 3.10+**
- **NLTK** â€“ for tokenization, stopword removal, stemming, and lemmatization
- **SpaCy** â€“ for advanced NLP tasks (optional)
- **scikit-learn** â€“ for machine learning models (Naive Bayes, Logistic Regression)
- **Wikipedia API** â€“ for collecting and parsing Wikipedia articles
- **WordNet** â€“ for lemmatization
- **Snowball** â€“ for stopword filtering

## ğŸ” Pipeline Overview

1. **Data Collection**
Â Â  - Fetch Wikipedia articles using the MediaWiki API
Â Â  - Label articles as geographic or non-geographic

2. **Text Preprocessing**
Â Â  - Tokenization
Â Â  - Stopword removal (Snowball)
Â Â  - Stemming (Porter) or Lemmatization (WordNet)

3. **Feature Extraction**
Â Â  - Bag of Words (BoW)
Â Â  - TF-IDF (optional)

4. **Model Training**
Â Â  - Naive Bayes
Â Â  - Logistic Regression (optional)

5. **Evaluation**
Â Â  - Accuracy, Precision, Recall, F1-score

6. **Deployment**
Â Â  - Code and documentation hosted on GitHub
Â Â  - Project description and pipeline included in this README

## ğŸš€ Getting Started

### Prerequisites

Install the required packages:

```bash
pip install -r requirements.txt
